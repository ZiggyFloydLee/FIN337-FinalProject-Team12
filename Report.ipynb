{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final report notebook for Team 12 in FIN 377. In this notebook we will explore our processes, modeling, as well as our conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start: These are all the packages we used throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data download and cleaning packages.\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "# We tried to fuzzymatch firms to their names, but ran into trouble and manually matched the CUSIPS\n",
    "# !pip install fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# Modeling packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Other Packages\n",
    "from tqdm import tqdm\n",
    "# We did not end up using wrds in our final process, but we thought it was a cool database and included it.\n",
    "# !pip install wrds\n",
    "import wrds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On to the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a lot of data in this project. It was very hard to clean, but we got there in the end.\n",
    "\n",
    "The goal was to end up with a merged dataset of 3 inputs:\n",
    "- Jay Ritter's SPAC data\n",
    "    - This data contains all SPAC mergers from 2016-2021.\n",
    "- Jay Ritter's IPO data\n",
    "    - This data contains all IPOs since 1975\n",
    "- CCM data\n",
    "    - This data contains 950 columns of observations on firms from 2000-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried a lot of methods to get this final dataset right.\n",
    "1. We started by merging on the different CUSIPs provided in each of the datasets.\n",
    "    - The problem with this method was that we could merge all of the data together, only to find there were 0 SPAC observations in the resulting dataset.\n",
    "    - We learned this is because the CUSIPs overlapped a bit between SPAC data and both IPO and CCM data, but none of the SPACs had CUSIPs that matched both datasets.\n",
    "2. FuzzyWuzzy -- We tried running a fuzzy match on the Company Names given in each of the datasets.\n",
    "    - This worked somewhat well, but was prone to innacurate readings.\n",
    "    - There was a company called \"Acquisition\" which the fuzzy match thought looked similar to any SPAC named \"Bob Joe Acquisition Corp\"\n",
    "    - When we deleted this company we ran a fuzzymatch that took 3 hours!\n",
    "    - It worked for about 60% of the data, but when combined with a confidence level of 90%, the fuzzy match ended up with only around 20% of SPACs.\n",
    "3. Manually adding merge keys\n",
    "    - We realized that the most accurate way to gather all the data would be to merge on CUSIP, all of the datasets have them, and although some are different, we could manually go through the SPAC dataset to match them to the IPO and CCM datasets.\n",
    "    - This worked! We ended up creating CCM_Cusip.csv which has CUSIP merge keys for both IPO data and CCM data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is the final merge we came up with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading data into DFs\n",
    "ipo_age_df = pd.read_csv('inputs/IPO-age(9).csv')\n",
    "cleaned_spacs = pd.read_csv('inputs/CCM_Cusip.csv')\n",
    "ccm_df = pd.read_csv('inputs/all_ccm_data.csv')\n",
    "# Narrowing the horizons to be more memory-friendly\n",
    "ipo_age_df = ipo_age_df[ipo_age_df['offer date'] > 20000000]\n",
    "ipo_age_df = ipo_age_df[ipo_age_df['offer date'] < 20190000]\n",
    "ipo_age_df = ipo_age_df.iloc[:, :-3]\n",
    "# Merging the SPAC and IPO data\n",
    "ziggymerge = pd.merge(ipo_age_df, cleaned_spacs, how='left', left_on='CUSIP', right_on='IPO_age_Cusip')\n",
    "# Merging the previous merge with CCM Data\n",
    "ziggymerge2 = pd.merge(left= ccm_df, right= ziggymerge, how='left', left_on='cusip', right_on='CCM_Cusip')\n",
    "ziggymerge2.to_csv('inputs/masterMerge.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then filtered this data based on what was streamlit-friendly and what numeric values we thought would be best to model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next up: Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
